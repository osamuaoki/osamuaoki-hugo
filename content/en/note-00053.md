---
title: Container with LXC/LXD (2)
date: 2023-11-12T00:00:00+00:00
categories:
  - desktop
  - english
tags:
  - lxc
slug: lxc-2
---

Here is a memo of me trying to use LXC/LXD on Debian 12 (`bookworm`).

This is follow-up to [Container with LXC/LXD (1)]({{<ref "note-00050.md" >}}).

## subuid and subgid on Debian with LXD

UID and GID:

* POSIX requires to cover: 0 - 2^16-1 = 65,535
  * 65,534 == nobody
* 64-bit Linux allows: 0 = 2^64-1 = 18,446,744,073,709,551,615

For Debian with 2 normal UID/GID, `/etc/subuid` and `/etc/subgid` are:
```
osamu:100000:65536
zzz:165536:65536
_lxd:231072:10000001
root:231072:10000001
```
This means:
* UID=0 in LXD container is mapped to UID=231072 on host.
* UID=1000 in LXD container is mapped to UID=232072 on host.

Please note, 165536 = (100000 + 65536) and 231072 = (165536 + 65536).

It seems that the values in these files changes with distributions and install
condition.  Debian uses `lxd` for user name and `_lxd` for group name while
other distribution seems to use `lxd` for both user and group name for LXD.

```
 $ grep lxd /etc/passwd
_lxd:x:129:138::/var/lib/lxd/:/bin/false
 $ grep lxd /etc/group
lxd:x:138:osamu
```

## Basic ID Mapping of LXD

I created and used the baseline `deb-12` image using `lxcinit -d -p 12`.

Then I started an instance `d0` with `lxc launch deb-12 d0`.

I started one root (UID=0) shell with `lxc exec d0 -- bash -i`, and one primary
user (UID=1000) shell with `lxc exec d0 -- sudo -u osamu bash -i` in a lxd
instance `d0`.

Then I started one primary user (UID=1000) shell on the host Gnome terminal,
and one root (UID=0) shell on the Linux terminal tty3 on the host system.
```
 $ ps aux
...
231072     18801  0.0  0.0   7588  3584 pts/1    Ss+  07:07   0:00 bash -i
...
232072     19122  0.0  0.0   7588  3584 pts/3    S+   07:08   0:00 bash -i
...
osamu      21311  1.2  0.0   8600  5376 pts/7    Ss   07:25   0:00 bash
...
root       22686  0.0  0.0   8468  5120 tty3     S+   07:33   0:00 -bash
...
````
Let's see `/proc/18801`, `/proc/19122`, `proc/21311`, and `proc/22686`.

| PID               | 18801  | 19122  | 21311 | 22686 |
|-------------------|--------|--------|-------|-------|
| UID (global)      | 231072 | 232072 |  1000 |     0 |
| UID in container  |      0 |   1000 | N/A   | N/A   |
| cgroup            | `0::/lxc.payload.d0/.lxc` | `0::/lxc.payload.d0/.lxc` | *1 | *2 |
| gid_map           | `0 231072 10000001` | `0 231072 10000001` | `0 0 4294967295` | `0 0 4294967295` |
| uid_map           | `0 231072 10000001` | `0 231072 10000001` | `0 0 4294967295` | `0 0 4294967295` |

- *1: `0::/user.slice/user-1000.slice/user@1000.service/app.slice/app-org.gnome.Terminal.slice/vte-spawn-15e39553-cc20-4c70-bfd5-f8b95e2c6b5a.scope`
- *2: `0::/user.slice/user-0.slice/session-11.scope`

## Customized ID maps for LXD instances

```
 $ mkdir -p ~/shared
 $ lxc config set d0 raw.idmap "both 1000 1000"
 $ lxc config show d0
architecture: x86_64
config:
  image.architecture: amd64
  image.description: Debian bookworm amd64 (cloud) (20231116_05:24)
  image.name: debian-bookworm-amd64-cloud-20231116_05:24
  image.os: debian
  image.release: bookworm
  image.serial: "20231116_05:24"
  image.variant: cloud
  raw.idmap: both 1000 1000
  volatile.base_image: d4df99a65c9df23e3c487a9ed7b51cfe9b13dc3da098b36f3bcd399837d4b27e
  volatile.cloud-init.instance-id: 2fc990c8-4846-477b-89c2-20d8f130ce86
  volatile.eth0.host_name: vethe6a419ae
  volatile.eth0.hwaddr: 00:16:3e:32:aa:6b
  volatile.idmap.base: "0"
  volatile.idmap.current: '[{"Isuid":true,"Isgid":false,"Hostid":231072,"Nsid":0,"Maprange":10000001},{"Isuid":false,"Isgid":true,"Hostid":231072,"Nsid":0,"Maprange":10000001}]'
  volatile.idmap.next: '[{"Isuid":true,"Isgid":false,"Hostid":231072,"Nsid":0,"Maprange":1000},{"Isuid":true,"Isgid":true,"Hostid":1000,"Nsid":1000,"Maprange":1},{"Isuid":true,"Isgid":false,"Hostid":232073,"Nsid":1001,"Maprange":9999000},{"Isuid":false,"Isgid":true,"Hostid":231072,"Nsid":0,"Maprange":1000},{"Isuid":true,"Isgid":true,"Hostid":1000,"Nsid":1000,"Maprange":1},{"Isuid":false,"Isgid":true,"Hostid":232073,"Nsid":1001,"Maprange":9999000}]'
  volatile.last_state.idmap: '[]'
  volatile.last_state.power: RUNNING
  volatile.uuid: c79ed4c4-dfef-48f5-9eeb-bbd86f20886a
devices: {}
ephemeral: false
profiles:
- default
stateful: false
description: ""
```


TBD

<!--

Let's start an instance `deb-test`.

```
 $ lxc init ..
```

## idmaps
note-00053
Different idmaps per container


These properties require a container reboot to take effect.

Custom idmaps
LXD also supports customizing bits of the idmap, e.g. to allow users to bind mount parts of the host’s file system into a container without the need for any UID-shifting file system. The per-container configuration key for this is raw.idmap, and looks like:

both 1000 1000
uid 50-60 500-510
gid 100000-110000 10000-20000
The first line configures both the UID and GID 1000 on the host to map to UID 1000 inside the container (this can be used for example to bind mount a user’s home directory into a container).

The second and third lines map only the UID or GID ranges into the container, respectively. The second entry per line is the source ID, i.e. the ID on the host, and the third entry is the range inside the container. These ranges must be the same size.

This property requires a container reboot to take effect.

https://discuss.linuxcontainers.org/t/need-id-mapping-root-and-user-in-container-to-same-user-in-host/15461
Need ID mapping root and user in container to same user in host
...
This is not possible AFAIK. So what I did?

lxc config set t7 raw.idmap "both 1000 0"
This maps my user in the host (1000:1000) to the root user into the LXD container.

What did I get?: my docker containers running inside the LXD container can manage devices and resources as root, and they can access all the directory binds that I need (owned by user 1000:1000 in the host).


https://gihyo.jp/admin/serial/01/ubuntu-recipe/0479
第479回
LXDコンテナとホストの間でファイルを共有する方法


ホストのディレクトリーツリーをbind mountする
名前空間による制限により、コンテナからホストのルートファイルシステムを直接見ることはできません。またAppArmorにより、コンテナ内部からのブロックファイルのmountは禁止されています。もしコンテナとホストの間、もしくは同じホスト上のコンテナ間でファイルやディレクトリを共有したい場合は、コンテナの設定でbind mountする方法が便利です。

$ lxc config device add sample share disk source=/srv/shared path=/srv/shared
上記のコマンドはホストの/srv/shared（source=オプション）を、コンテナ内部の/srv/shared（path=オプション）としてbind mountします。つまりホストとコンテナで/srv/sharedを共有できるというわけです。

lxc configによる設定は永続的に反映されますので、コンテナの再起動を行ってもそのまま残っています。

$ lxc config show sample
（中略）
devices:
  nfsdir:
    path: /srv/shared
    source: /srv/shared
    type: disk
（後略）
この方法のメリットは、ホスト側のファイルシステムに関係なく同じ方法でコンテナと共有できることです。たとえばコンテナの内部からNFSディレクトリをmountする場合も、一度ホスト側でNFSディレクトリをmountしておけば、それをそのままコンテナにbind mountできるのです。よって同じホスト上のコンテナ間だけでなく、ネットワーク越しのコンテナ間ともファイルやディレクトリを共有できます。

ただしlxc fileと異なり、UID等のマッピングの変更は行いません。よってコンテナの一般ユーザーで作ったファイルやディレクトリのオーナーは、ホストから見るととても大きなUIDを持っていることになります。

ホストとコンテナでUID等を一致させる
bind mountによる共有ディレクトリを使う場合、ホストとコンテナで一般ユーザーのUID等を一致させたほうが便利です。つまりコンテナのUID=1000はホストでもUID=1000にします。もちろんrootを一致させるとそれはとどのつまり「ほぼ特権コンテナ」となりますので、非特権コンテナにしておく意味が薄れます。あくまで「普段使うユーザー」のみUIDを一致させるのです。理想的にはそのユーザーはsudoグループに入っていないほうがいいでしょう。

UID等のマッピングは/etc/subuid、/etc/subgidファイルで管理しています。UID=1000、GID=1000のユーザーとグループはホストとコンテナで一致したい場合は次のように設定します。

$ echo "root:1000:1" | sudo tee -a /etc/subuid
$ echo "root:1000:1" | sudo tee -a /etc/subgid
このうちrootは今回設定したUID等のマッピングを許可するユーザーです。LXDの場合はrootがコンテナ起動時にマッピングを行うのでrootにしています。書式は「USER:START:COUNT」です。上記設定の場合は、「⁠ID=1000から1つ」になります。もちろん「1」を「100」にして複数のUIDをマッピングすることも可能です。

コンテナ内部のマッピングに関する設定は、コンテナごとに行います。

$ lxc config set sample raw.idmap 'both 1000 1000'
「both」はUIDとGID両方に同じ値を設定するパラメーターです。書式は「both HOST_ID CONTAINER_ID」となります。HOST_IDにはホスト上のUID/GIDを、CONTAINER_IDにはHOST_IDをコンテナの中にマップした時のIDを記述します。言い換えると「ホスト上のUID/GID 1000をコンテナ上のUID/GID 1000として扱う」ということです。

もしUID/GIDを異なる値にしたい場合はbothのかわりにuidやgidを使います。

ホスト上のUID=1010をコンテナ上ではUID=1000として扱いたい場合
uid 1010 1000

ホスト上のGID=1011をコンテナ上ではGID=1000として扱いたい場合
gid 1011 1000
uidとgidを同時に設定したい場合は、改行でつなぎます。しかしながらlxc configコマンドは直接改行を扱えません。そこで設定内容を標準入力から受け付けるようにして、そこにパイプで設定内容を流し込みます。

$ echo -e "uid 1010 1000\ngid 1011 1000" | lxc config set xenial raw.idmap -
ハイフンをつないで範囲を指定することも可能です。この場合、マッピング元とマッピング先の個数が同じになるようにしてください。

変更した設定を反映するには、一度そのコンテナを再起動します。冒頭と同じようにプロセスのUIDをコンテナとホストで比べたら、UID=0はマッピングされているものの、UID=1000はコンテナとホストで一致していることがわかるはずです。

これらを組み合わせれば、ホスト・コンテナ間のファイルやディレクトリのアクセス権をそれなりにコントロールできることでしょう。
---------------
https://stgraber.org/2017/06/15/custom-user-mappings-in-lxd-containers/
Introduction
As you may know, LXD uses unprivileged containers by default.
The difference between an unprivileged container and a privileged one is whether the root user in the container is the “real” root user (uid 0 at the kernel level).

The way unprivileged containers are created is by taking a set of normal UIDs and GIDs from the host, usually at least 65536 of each (to be POSIX compliant) and mapping those into the container.

The most common example and what most LXD users will end up with by default is a map of 65536 UIDs and GIDs, with a host base id of 100000. This means that root in the container (uid 0) will be mapped to the host uid 100000 and uid 65535 in the container will be mapped to uid 165535 on the host. UID/GID 65536 and higher in the container aren’t mapped and will return an error if you attempt to use them.

From a security point of view, that means that anything which is not owned by the users and groups mapped into the container will be inaccessible. Any such resource will show up as being owned by uid/gid “-1” (rendered as 65534 or nobody/nogroup in userspace). It also means that should there be a way to escape the container, even root in the container would find itself with just as much privileges on the host as a nobody user.

LXD does offer a number of options related to unprivileged configuration:

Increasing the size of the default uid/gid map
Setting up per-container maps
Punching holes into the map to expose host users and groups
Increasing the size of the default map
As mentioned above, in most cases, LXD will have a default map that’s made of 65536 uids/gids.

In most cases you won’t have to change that. There are however a few cases where you may have to:

You need access to uid/gid higher than 65535.
This is most common when using network authentication inside of your containers.
You want to use per-container maps.
In which case you’ll need 65536 available uid/gid per container.
You want to punch some holes in your container’s map and need access to host uids/gids.
The default map is usually controlled by the “shadow” set of utilities and files. On systems where that’s the case, the “/etc/subuid” and “/etc/subgid” files are used to configure those maps.

On systems that do not have a recent enough version of the “shadow” package. LXD will assume that it doesn’t have to share uid/gid ranges with anything else and will therefore assume control of a billion uids and gids, starting at the host uid/gid 100000.

But the common case, is a system with a recent version of shadow.
An example of what the configuration may look like is:

stgraber@castiana:~$ cat /etc/subuid
lxd:100000:65536
root:100000:65536

stgraber@castiana:~$ cat /etc/subgid
lxd:100000:65536
root:100000:65536
The maps for “lxd” and “root” should always be kept in sync. LXD itself is restricted by the “root” allocation. The “lxd” entry is used to track what needs to be removed if LXD is uninstalled.

Now if you want to increase the size of the map available to LXD. Simply edit both of the files and bump the last value from 65536 to whatever size you need. I tend to bump it to a billion just so I don’t ever have to think about it again:

stgraber@castiana:~$ cat /etc/subuid
lxd:100000:1000000000
root:100000:1000000000

stgraber@castiana:~$ cat /etc/subgid
lxd:100000:1000000000
root:100000:100000000
After altering those files, you need to restart LXD to have it detect the new map:

root@vorash:~# systemctl restart lxd
root@vorash:~# cat /var/log/lxd/lxd.log
lvl=info msg="LXD 2.14 is starting in normal mode" path=/var/lib/lxd t=2017-06-14T21:21:13+0000
lvl=warn msg="CGroup memory swap accounting is disabled, swap limits will be ignored." t=2017-06-14T21:21:13+0000
lvl=info msg="Kernel uid/gid map:" t=2017-06-14T21:21:13+0000
lvl=info msg=" - u 0 0 4294967295" t=2017-06-14T21:21:13+0000
lvl=info msg=" - g 0 0 4294967295" t=2017-06-14T21:21:13+0000
lvl=info msg="Configured LXD uid/gid map:" t=2017-06-14T21:21:13+0000
lvl=info msg=" - u 0 1000000 1000000000" t=2017-06-14T21:21:13+0000
lvl=info msg=" - g 0 1000000 1000000000" t=2017-06-14T21:21:13+0000
lvl=info msg="Connecting to a remote simplestreams server" t=2017-06-14T21:21:13+0000
lvl=info msg="Expiring log files" t=2017-06-14T21:21:13+0000
lvl=info msg="Done expiring log files" t=2017-06-14T21:21:13+0000
lvl=info msg="Starting /dev/lxd handler" t=2017-06-14T21:21:13+0000
lvl=info msg="LXD is socket activated" t=2017-06-14T21:21:13+0000
lvl=info msg="REST API daemon:" t=2017-06-14T21:21:13+0000
lvl=info msg=" - binding Unix socket" socket=/var/lib/lxd/unix.socket t=2017-06-14T21:21:13+0000
lvl=info msg=" - binding TCP socket" socket=[::]:8443 t=2017-06-14T21:21:13+0000
lvl=info msg="Pruning expired images" t=2017-06-14T21:21:13+0000
lvl=info msg="Updating images" t=2017-06-14T21:21:13+0000
lvl=info msg="Done pruning expired images" t=2017-06-14T21:21:13+0000
lvl=info msg="Done updating images" t=2017-06-14T21:21:13+0000
root@vorash:~#
As you can see, the configured map is logged at LXD startup and can be used to confirm that the reconfiguration worked as expected.

You’ll then need to restart your containers to have them start using your newly expanded map.

Per container maps
Provided that you have a sufficient amount of uid/gid allocated to LXD, you can configure your containers to use their own, non-overlapping allocation of uids and gids.

This can be useful for two reasons:

You are running software which alters kernel resource ulimits.
Those user-specific limits are tied to a kernel uid and will cross container boundaries leading to hard to debug issues where one container can perform an action but all others are then unable to do the same.
You want to know that should there be a way for someone in one of your containers to somehow get access to the host that they still won’t be able to access or interact with any of the other containers.
The main downsides to using this feature are:

It’s somewhat wasteful with using 65536 uids and gids per container.
That being said, you’d still be able to run over 60000 isolated containers before running out of system uids and gids.
It’s effectively impossible to share storage between two isolated containers as everything written by one will be seen as -1 by the other. There is ongoing work around virtual filesystems in the kernel that will eventually let us get rid of that limitation.
To have a container use its own distinct map, simply run:

stgraber@castiana:~$ lxc config set test security.idmap.isolated true
stgraber@castiana:~$ lxc restart test
stgraber@castiana:~$ lxc config get test volatile.last_state.idmap
[{"Isuid":true,"Isgid":false,"Hostid":165536,"Nsid":0,"Maprange":65536},{"Isuid":false,"Isgid":true,"Hostid":165536,"Nsid":0,"Maprange":65536}]
The restart step is needed to have LXD remap the entire filesystem of the container to its new map.
Note that this step will take a varying amount of time depending on the number of files in the container and the speed of your storage.

As can be seen above, after restart, the container is shown to have its own map of 65536 uids/gids.

If you want LXD to allocate more than the default 65536 uids/gids to an isolated container, you can bump the size of the allocation with:

stgraber@castiana:~$ lxc config set test security.idmap.size 200000
stgraber@castiana:~$ lxc restart test
stgraber@castiana:~$ lxc config get test volatile.last_state.idmap
[{"Isuid":true,"Isgid":false,"Hostid":165536,"Nsid":0,"Maprange":200000},{"Isuid":false,"Isgid":true,"Hostid":165536,"Nsid":0,"Maprange":200000}]
If you’re trying to allocate more uids/gids than are left in LXD’s allocation, LXD will let you know:

stgraber@castiana:~$ lxc config set test security.idmap.size 2000000000
error: Not enough uid/gid available for the container.
Direct user/group mapping
The fact that all uids/gids in an unprivileged container are mapped to a normally unused range on the host means that sharing of data between host and container is effectively impossible.

Now, what if you want to share your user’s home directory with a container?

The obvious answer to that is to define a new “disk” entry in LXD which passes your home directory to the container:

stgraber@castiana:~$ lxc config device add test home disk source=/home/stgraber path=/home/ubuntu
Device home added to test
So that was pretty easy, but did it work?

stgraber@castiana:~$ lxc exec test -- bash
root@test:~# ls -lh /home/
total 529K
drwx--x--x 45 nobody nogroup 84 Jun 14 20:06 ubuntu
No. The mount is clearly there, but it’s completely inaccessible to the container.
To fix that, we need to take a few extra steps:

Allow LXD’s use of our user uid and gid
Restart LXD to have it load the new map
Set a custom map for our container
Restart the container to have the new map apply
stgraber@castiana:~$ printf "lxd:$(id -u):1\nroot:$(id -u):1\n" | sudo tee -a /etc/subuid
lxd:201105:1
root:201105:1

stgraber@castiana:~$ printf "lxd:$(id -g):1\nroot:$(id -g):1\n" | sudo tee -a /etc/subgid
lxd:200512:1
root:200512:1

stgraber@castiana:~$ sudo systemctl restart lxd

stgraber@castiana:~$ printf "uid $(id -u) 1000\ngid $(id -g) 1000" | lxc config set test raw.idmap -

stgraber@castiana:~$ lxc restart test
At which point, things should be working in the container:

stgraber@castiana:~$ lxc exec test -- su ubuntu -l
ubuntu@test:~$ ls -lh
total 119K
drwxr-xr-x 5  ubuntu ubuntu 8 Feb 18 2016 data
drwxr-x--- 4  ubuntu ubuntu 6 Jun 13 17:05 Desktop
drwxr-xr-x 3  ubuntu ubuntu 28 Jun 13 20:09 Downloads
drwx------ 84 ubuntu ubuntu 84 Sep 14 2016 Maildir
drwxr-xr-x 4  ubuntu ubuntu 4 May 20 15:38 snap
ubuntu@test:~$

Conclusion
User namespaces, the kernel feature that makes those uid/gid mappings possible is a very powerful tool which finally made containers on Linux safe by design. It is however not the easiest thing to wrap your head around and all of that uid/gid map math can quickly become a major issue.

In LXD we’ve tried to expose just enough of those underlying features to be useful to our users while doing the actual mapping math internally. This makes things like the direct user/group mapping above significantly easier than it otherwise would be.

Going forward, we’re very interested in some of the work around uid/gid remapping at the filesystem level, this would let us decouple the on-disk user/group map from that used for processes, making it possible to share data between differently mapped containers and alter the various maps without needing to also remap the entire filesystem.

Extra information
The main LXD website is at: https://linuxcontainers.org/lxd
Development happens on Github at: https://github.com/lxc/lxd
Discussion forun: https://discuss.linuxcontainers.org
Mailing-list support happens on: https://lists.linuxcontainers.org
IRC support happens in: #lxcontainers on irc.freenode.net
Try LXD online: https://linuxcontainers.org/lxd/try-it


----
https://stackoverflow.com/questions/71738168/linux-container-failed-to-set-up-id-mapping


your mappings look quite wrong... syntax should be :

/etc/subuid

[local unprivileged user on the host that will run the container]:[uid on the host start at(this is inexisting uid on the host and it is ok)]:[number/range of uid to map]

e.g : toto:100000:65535

/etc/subuid

[local unprivileged group on the host that will run the container]:[guid on the host start at(this is inexisting gid on the host and it is ok)]:[number/range of uid to map]

e.g : toto:100000:65535

config

then, in the config file of the container, if you want to restrict the access further, you can do some specific mappings:

lxc.idmap = u 0 100000 1

lxc.idmap = g 0 100000 1

lxc.idmap = u 33 100033 1

lxc.idmap = g 33 100033 1

will map the uid and gid 0 (root) in the container to uid and gid 100000 on the host

will map the uid and gid 33 (www-data) in the container to uid and gid 100033 on the host

--------------

LXDのコンテナ中でホストのホームディレクトリを読み書きする方法
https://qiita.com/m-shibata/items/2969ab84bab9235d25f0
https://ubuntu.com/blog/mounting-your-home-directory-in-lxd
Mounting your home directory in LXD
As of LXD stable 2.0.8 and feature release 2.6, LXD has support for various UID and GID map related manipulaions. A common question is: “How do I bind-mount my home directory into a container?” and before the answer was “well, it’s complicated but you can do it; it’s slightly less complicated if you do it in privleged containers”. However, with this feature, now you can do it very easily in unprivileged containers.

First, find out your uid on the host:

$ id
uid=1000(tycho) gid=1000(tycho) groups=1000(tycho),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),112(lpadmin),124(sambashare),129(libvirtd),149(lxd),150(sbuild)
On standard Ubuntu hosts, the uid of the first user is 1000. Now, we need to allow LXD to remap to remap this id; you’ll need an additional entry for root to do this:

$ echo 'root:1000:1' | sudo tee -a /etc/subuid /etc/subgid
Now, create a container, and set the idmap up to map both uid and gid 1000 to uid and gid 1000 inside the container.

$ lxc init ubuntu-daily:z zesty
Creating zesty

$ lxc config set zesty raw.idmap 'both 1000 1000'
Finally, set up your home directory to be mounted in the container:

$ lxc config device add zesty homedir disk source=/home/tycho path=/home/ubuntu
And leave an insightful message for users of the container:

$ echo 'meshuggah rocks' >> message
Finally, start your container and read the message:

$ lxc start zesty
$ lxc exec zesty cat /home/ubuntu/message
meshuggah rocks
And enjoy the insight offered to you by your home directory 🙂


Sunday, August 28, 2022

-->

See more in [Container with LXC/LXD (3)]({{<ref "note-00054.md" >}}).

<!-- vim: set sw=4 sts=4 ai si et tw=79 ft=markdown: -->
