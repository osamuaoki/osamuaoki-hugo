---
title: Container with LXC/LXD (2)
date: 2023-11-12T00:00:00+00:00
categories:
  - desktop
  - english
tags:
  - lxc
slug: lxc-2
---

Here is a memo of me trying to use LXC/LXD on Debian 12 (`bookworm`).

This is follow-up to [Container with LXC/LXD (1)]({{<ref "note-00050.md" >}}).

## subuid and subgid on Debian with LXD

UID and GID:

* POSIX requires to cover: 0 - 2^16-1 = 65,535
  * 65,534 == nobody
* 64-bit Linux allows: 0 = 2^64-1 = 18,446,744,073,709,551,615

For Debian with 2 normal UID/GID, `/etc/subuid` and `/etc/subgid` are:
```
osamu:100000:65536
zzz:165536:65536
_lxd:231072:10000001
root:231072:10000001
```
This means:
* UID=0 in LXD container is mapped to UID=231072 on host.
* UID=1000 in LXD container is mapped to UID=232072 on host.

Please note, 165536 = (100000 + 65536) and 231072 = (165536 + 65536).

It seems that the values in these files changes with distributions and install
condition.  Debian uses `lxd` for user name and `_lxd` for group name while
other distribution seems to use `lxd` for both user and group name for LXD.

```
 $ grep lxd /etc/passwd
_lxd:x:129:138::/var/lib/lxd/:/bin/false
 $ grep lxd /etc/group
lxd:x:138:osamu
```

## Basic ID Mapping of LXD

I created and used the baseline `deb-12` image using `lxcinit -d -p 12`.

Then I started an instance `d0` with `lxc launch deb-12 d0`.

I started one root (UID=0) shell with `lxc exec d0 -- bash -i`, and one primary
user (UID=1000) shell with `lxc exec d0 -- sudo -u osamu bash -i` in a lxd
instance `d0`.

Then I started one primary user (UID=1000) shell on the host Gnome terminal,
and one root (UID=0) shell on the Linux terminal tty3 on the host system.
```
 $ ps aux
...
231072     18801  0.0  0.0   7588  3584 pts/1    Ss+  07:07   0:00 bash -i
...
232072     19122  0.0  0.0   7588  3584 pts/3    S+   07:08   0:00 bash -i
...
osamu      21311  1.2  0.0   8600  5376 pts/7    Ss   07:25   0:00 bash
...
root       22686  0.0  0.0   8468  5120 tty3     S+   07:33   0:00 -bash
...
````
Let's see `/proc/18801`, `/proc/19122`, `proc/21311`, and `proc/22686`.

| PID               | 18801  | 19122  | 21311 | 22686 |
|-------------------|--------|--------|-------|-------|
| UID (global)      | 231072 | 232072 |  1000 |     0 |
| UID in container  |      0 |   1000 | N/A   | N/A   |
| cgroup            | `0::/lxc.payload.d0/.lxc` | `0::/lxc.payload.d0/.lxc` | *1 | *2 |
| gid_map           | `0 231072 10000001` | `0 231072 10000001` | `0 0 4294967295` | `0 0 4294967295` |
| uid_map           | `0 231072 10000001` | `0 231072 10000001` | `0 0 4294967295` | `0 0 4294967295` |

- *1: `0::/user.slice/user-1000.slice/user@1000.service/app.slice/app-org.gnome.Terminal.slice/vte-spawn-15e39553-cc20-4c70-bfd5-f8b95e2c6b5a.scope`
- *2: `0::/user.slice/user-0.slice/session-11.scope`

## Customized ID maps for LXD instances

```
 $ mkdir -p ~/shared
 $ lxc config set d0 raw.idmap "both 1000 1000"
 $ lxc config show d0
architecture: x86_64
config:
  image.architecture: amd64
  image.description: Debian bookworm amd64 (cloud) (20231116_05:24)
  image.name: debian-bookworm-amd64-cloud-20231116_05:24
  image.os: debian
  image.release: bookworm
  image.serial: "20231116_05:24"
  image.variant: cloud
  raw.idmap: both 1000 1000
  volatile.base_image: d4df99a65c9df23e3c487a9ed7b51cfe9b13dc3da098b36f3bcd399837d4b27e
  volatile.cloud-init.instance-id: 2fc990c8-4846-477b-89c2-20d8f130ce86
  volatile.eth0.host_name: vethe6a419ae
  volatile.eth0.hwaddr: 00:16:3e:32:aa:6b
  volatile.idmap.base: "0"
  volatile.idmap.current: '[{"Isuid":true,"Isgid":false,"Hostid":231072,"Nsid":0,"Maprange":10000001},{"Isuid":false,"Isgid":true,"Hostid":231072,"Nsid":0,"Maprange":10000001}]'
  volatile.idmap.next: '[{"Isuid":true,"Isgid":false,"Hostid":231072,"Nsid":0,"Maprange":1000},{"Isuid":true,"Isgid":true,"Hostid":1000,"Nsid":1000,"Maprange":1},{"Isuid":true,"Isgid":false,"Hostid":232073,"Nsid":1001,"Maprange":9999000},{"Isuid":false,"Isgid":true,"Hostid":231072,"Nsid":0,"Maprange":1000},{"Isuid":true,"Isgid":true,"Hostid":1000,"Nsid":1000,"Maprange":1},{"Isuid":false,"Isgid":true,"Hostid":232073,"Nsid":1001,"Maprange":9999000}]'
  volatile.last_state.idmap: '[]'
  volatile.last_state.power: RUNNING
  volatile.uuid: c79ed4c4-dfef-48f5-9eeb-bbd86f20886a
devices: {}
ephemeral: false
profiles:
- default
stateful: false
description: ""
```


TBD

<!--

Let's start an instance `deb-test`.

```
 $ lxc init ..
```

## idmaps
note-00053
Different idmaps per container


These properties require a container reboot to take effect.

Custom idmaps
LXD also supports customizing bits of the idmap, e.g. to allow users to bind mount parts of the hostâ€™s file system into a container without the need for any UID-shifting file system. The per-container configuration key for this is raw.idmap, and looks like:

both 1000 1000
uid 50-60 500-510
gid 100000-110000 10000-20000
The first line configures both the UID and GID 1000 on the host to map to UID 1000 inside the container (this can be used for example to bind mount a userâ€™s home directory into a container).

The second and third lines map only the UID or GID ranges into the container, respectively. The second entry per line is the source ID, i.e. the ID on the host, and the third entry is the range inside the container. These ranges must be the same size.

This property requires a container reboot to take effect.

https://discuss.linuxcontainers.org/t/need-id-mapping-root-and-user-in-container-to-same-user-in-host/15461
Need ID mapping root and user in container to same user in host
...
This is not possible AFAIK. So what I did?

lxc config set t7 raw.idmap "both 1000 0"
This maps my user in the host (1000:1000) to the root user into the LXD container.

What did I get?: my docker containers running inside the LXD container can manage devices and resources as root, and they can access all the directory binds that I need (owned by user 1000:1000 in the host).


https://gihyo.jp/admin/serial/01/ubuntu-recipe/0479
ç¬¬479å›ž
LXDã‚³ãƒ³ãƒ†ãƒŠã¨ãƒ›ã‚¹ãƒˆã®é–“ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…±æœ‰ã™ã‚‹æ–¹æ³•


ãƒ›ã‚¹ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ¼ãƒ„ãƒªãƒ¼ã‚’bind mountã™ã‚‹
åå‰ç©ºé–“ã«ã‚ˆã‚‹åˆ¶é™ã«ã‚ˆã‚Šã€ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰ãƒ›ã‚¹ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚’ç›´æŽ¥è¦‹ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚ã¾ãŸAppArmorã«ã‚ˆã‚Šã€ã‚³ãƒ³ãƒ†ãƒŠå†…éƒ¨ã‹ã‚‰ã®ãƒ–ãƒ­ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®mountã¯ç¦æ­¢ã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚‚ã—ã‚³ãƒ³ãƒ†ãƒŠã¨ãƒ›ã‚¹ãƒˆã®é–“ã€ã‚‚ã—ãã¯åŒã˜ãƒ›ã‚¹ãƒˆä¸Šã®ã‚³ãƒ³ãƒ†ãƒŠé–“ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å…±æœ‰ã—ãŸã„å ´åˆã¯ã€ã‚³ãƒ³ãƒ†ãƒŠã®è¨­å®šã§bind mountã™ã‚‹æ–¹æ³•ãŒä¾¿åˆ©ã§ã™ã€‚

$ lxc config device add sample share disk source=/srv/shared path=/srv/shared
ä¸Šè¨˜ã®ã‚³ãƒžãƒ³ãƒ‰ã¯ãƒ›ã‚¹ãƒˆã®/srv/sharedï¼ˆsource=ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã‚’ã€ã‚³ãƒ³ãƒ†ãƒŠå†…éƒ¨ã®/srv/sharedï¼ˆpath=ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã¨ã—ã¦bind mountã—ã¾ã™ã€‚ã¤ã¾ã‚Šãƒ›ã‚¹ãƒˆã¨ã‚³ãƒ³ãƒ†ãƒŠã§/srv/sharedã‚’å…±æœ‰ã§ãã‚‹ã¨ã„ã†ã‚ã‘ã§ã™ã€‚

lxc configã«ã‚ˆã‚‹è¨­å®šã¯æ°¸ç¶šçš„ã«åæ˜ ã•ã‚Œã¾ã™ã®ã§ã€ã‚³ãƒ³ãƒ†ãƒŠã®å†èµ·å‹•ã‚’è¡Œã£ã¦ã‚‚ãã®ã¾ã¾æ®‹ã£ã¦ã„ã¾ã™ã€‚

$ lxc config show sample
ï¼ˆä¸­ç•¥ï¼‰
devices:
  nfsdir:
    path: /srv/shared
    source: /srv/shared
    type: disk
ï¼ˆå¾Œç•¥ï¼‰
ã“ã®æ–¹æ³•ã®ãƒ¡ãƒªãƒƒãƒˆã¯ã€ãƒ›ã‚¹ãƒˆå´ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«é–¢ä¿‚ãªãåŒã˜æ–¹æ³•ã§ã‚³ãƒ³ãƒ†ãƒŠã¨å…±æœ‰ã§ãã‚‹ã“ã¨ã§ã™ã€‚ãŸã¨ãˆã°ã‚³ãƒ³ãƒ†ãƒŠã®å†…éƒ¨ã‹ã‚‰NFSãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’mountã™ã‚‹å ´åˆã‚‚ã€ä¸€åº¦ãƒ›ã‚¹ãƒˆå´ã§NFSãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’mountã—ã¦ãŠã‘ã°ã€ãã‚Œã‚’ãã®ã¾ã¾ã‚³ãƒ³ãƒ†ãƒŠã«bind mountã§ãã‚‹ã®ã§ã™ã€‚ã‚ˆã£ã¦åŒã˜ãƒ›ã‚¹ãƒˆä¸Šã®ã‚³ãƒ³ãƒ†ãƒŠé–“ã ã‘ã§ãªãã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¶Šã—ã®ã‚³ãƒ³ãƒ†ãƒŠé–“ã¨ã‚‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å…±æœ‰ã§ãã¾ã™ã€‚

ãŸã ã—lxc fileã¨ç•°ãªã‚Šã€UIDç­‰ã®ãƒžãƒƒãƒ”ãƒ³ã‚°ã®å¤‰æ›´ã¯è¡Œã„ã¾ã›ã‚“ã€‚ã‚ˆã£ã¦ã‚³ãƒ³ãƒ†ãƒŠã®ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ä½œã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚ªãƒ¼ãƒŠãƒ¼ã¯ã€ãƒ›ã‚¹ãƒˆã‹ã‚‰è¦‹ã‚‹ã¨ã¨ã¦ã‚‚å¤§ããªUIDã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚

ãƒ›ã‚¹ãƒˆã¨ã‚³ãƒ³ãƒ†ãƒŠã§UIDç­‰ã‚’ä¸€è‡´ã•ã›ã‚‹
bind mountã«ã‚ˆã‚‹å…±æœ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ã†å ´åˆã€ãƒ›ã‚¹ãƒˆã¨ã‚³ãƒ³ãƒ†ãƒŠã§ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®UIDç­‰ã‚’ä¸€è‡´ã•ã›ãŸã»ã†ãŒä¾¿åˆ©ã§ã™ã€‚ã¤ã¾ã‚Šã‚³ãƒ³ãƒ†ãƒŠã®UID=1000ã¯ãƒ›ã‚¹ãƒˆã§ã‚‚UID=1000ã«ã—ã¾ã™ã€‚ã‚‚ã¡ã‚ã‚“rootã‚’ä¸€è‡´ã•ã›ã‚‹ã¨ãã‚Œã¯ã¨ã©ã®ã¤ã¾ã‚Šã€Œã»ã¼ç‰¹æ¨©ã‚³ãƒ³ãƒ†ãƒŠã€ã¨ãªã‚Šã¾ã™ã®ã§ã€éžç‰¹æ¨©ã‚³ãƒ³ãƒ†ãƒŠã«ã—ã¦ãŠãæ„å‘³ãŒè–„ã‚Œã¾ã™ã€‚ã‚ãã¾ã§ã€Œæ™®æ®µä½¿ã†ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€ã®ã¿UIDã‚’ä¸€è‡´ã•ã›ã‚‹ã®ã§ã™ã€‚ç†æƒ³çš„ã«ã¯ãã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯sudoã‚°ãƒ«ãƒ¼ãƒ—ã«å…¥ã£ã¦ã„ãªã„ã»ã†ãŒã„ã„ã§ã—ã‚‡ã†ã€‚

UIDç­‰ã®ãƒžãƒƒãƒ”ãƒ³ã‚°ã¯/etc/subuidã€/etc/subgidãƒ•ã‚¡ã‚¤ãƒ«ã§ç®¡ç†ã—ã¦ã„ã¾ã™ã€‚UID=1000ã€GID=1000ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚°ãƒ«ãƒ¼ãƒ—ã¯ãƒ›ã‚¹ãƒˆã¨ã‚³ãƒ³ãƒ†ãƒŠã§ä¸€è‡´ã—ãŸã„å ´åˆã¯æ¬¡ã®ã‚ˆã†ã«è¨­å®šã—ã¾ã™ã€‚

$ echo "root:1000:1" | sudo tee -a /etc/subuid
$ echo "root:1000:1" | sudo tee -a /etc/subgid
ã“ã®ã†ã¡rootã¯ä»Šå›žè¨­å®šã—ãŸUIDç­‰ã®ãƒžãƒƒãƒ”ãƒ³ã‚°ã‚’è¨±å¯ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã™ã€‚LXDã®å ´åˆã¯rootãŒã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•æ™‚ã«ãƒžãƒƒãƒ”ãƒ³ã‚°ã‚’è¡Œã†ã®ã§rootã«ã—ã¦ã„ã¾ã™ã€‚æ›¸å¼ã¯ã€ŒUSER:START:COUNTã€ã§ã™ã€‚ä¸Šè¨˜è¨­å®šã®å ´åˆã¯ã€ã€Œâ ID=1000ã‹ã‚‰1ã¤ã€ã«ãªã‚Šã¾ã™ã€‚ã‚‚ã¡ã‚ã‚“ã€Œ1ã€ã‚’ã€Œ100ã€ã«ã—ã¦è¤‡æ•°ã®UIDã‚’ãƒžãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚

ã‚³ãƒ³ãƒ†ãƒŠå†…éƒ¨ã®ãƒžãƒƒãƒ”ãƒ³ã‚°ã«é–¢ã™ã‚‹è¨­å®šã¯ã€ã‚³ãƒ³ãƒ†ãƒŠã”ã¨ã«è¡Œã„ã¾ã™ã€‚

$ lxc config set sample raw.idmap 'both 1000 1000'
ã€Œbothã€ã¯UIDã¨GIDä¸¡æ–¹ã«åŒã˜å€¤ã‚’è¨­å®šã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚æ›¸å¼ã¯ã€Œboth HOST_ID CONTAINER_IDã€ã¨ãªã‚Šã¾ã™ã€‚HOST_IDã«ã¯ãƒ›ã‚¹ãƒˆä¸Šã®UID/GIDã‚’ã€CONTAINER_IDã«ã¯HOST_IDã‚’ã‚³ãƒ³ãƒ†ãƒŠã®ä¸­ã«ãƒžãƒƒãƒ—ã—ãŸæ™‚ã®IDã‚’è¨˜è¿°ã—ã¾ã™ã€‚è¨€ã„æ›ãˆã‚‹ã¨ã€Œãƒ›ã‚¹ãƒˆä¸Šã®UID/GID 1000ã‚’ã‚³ãƒ³ãƒ†ãƒŠä¸Šã®UID/GID 1000ã¨ã—ã¦æ‰±ã†ã€ã¨ã„ã†ã“ã¨ã§ã™ã€‚

ã‚‚ã—UID/GIDã‚’ç•°ãªã‚‹å€¤ã«ã—ãŸã„å ´åˆã¯bothã®ã‹ã‚ã‚Šã«uidã‚„gidã‚’ä½¿ã„ã¾ã™ã€‚

ãƒ›ã‚¹ãƒˆä¸Šã®UID=1010ã‚’ã‚³ãƒ³ãƒ†ãƒŠä¸Šã§ã¯UID=1000ã¨ã—ã¦æ‰±ã„ãŸã„å ´åˆ
uid 1010 1000

ãƒ›ã‚¹ãƒˆä¸Šã®GID=1011ã‚’ã‚³ãƒ³ãƒ†ãƒŠä¸Šã§ã¯GID=1000ã¨ã—ã¦æ‰±ã„ãŸã„å ´åˆ
gid 1011 1000
uidã¨gidã‚’åŒæ™‚ã«è¨­å®šã—ãŸã„å ´åˆã¯ã€æ”¹è¡Œã§ã¤ãªãŽã¾ã™ã€‚ã—ã‹ã—ãªãŒã‚‰lxc configã‚³ãƒžãƒ³ãƒ‰ã¯ç›´æŽ¥æ”¹è¡Œã‚’æ‰±ãˆã¾ã›ã‚“ã€‚ãã“ã§è¨­å®šå†…å®¹ã‚’æ¨™æº–å…¥åŠ›ã‹ã‚‰å—ã‘ä»˜ã‘ã‚‹ã‚ˆã†ã«ã—ã¦ã€ãã“ã«ãƒ‘ã‚¤ãƒ—ã§è¨­å®šå†…å®¹ã‚’æµã—è¾¼ã¿ã¾ã™ã€‚

$ echo -e "uid 1010 1000\ngid 1011 1000" | lxc config set xenial raw.idmap -
ãƒã‚¤ãƒ•ãƒ³ã‚’ã¤ãªã„ã§ç¯„å›²ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚ã“ã®å ´åˆã€ãƒžãƒƒãƒ”ãƒ³ã‚°å…ƒã¨ãƒžãƒƒãƒ”ãƒ³ã‚°å…ˆã®å€‹æ•°ãŒåŒã˜ã«ãªã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚

å¤‰æ›´ã—ãŸè¨­å®šã‚’åæ˜ ã™ã‚‹ã«ã¯ã€ä¸€åº¦ãã®ã‚³ãƒ³ãƒ†ãƒŠã‚’å†èµ·å‹•ã—ã¾ã™ã€‚å†’é ­ã¨åŒã˜ã‚ˆã†ã«ãƒ—ãƒ­ã‚»ã‚¹ã®UIDã‚’ã‚³ãƒ³ãƒ†ãƒŠã¨ãƒ›ã‚¹ãƒˆã§æ¯”ã¹ãŸã‚‰ã€UID=0ã¯ãƒžãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã®ã€UID=1000ã¯ã‚³ãƒ³ãƒ†ãƒŠã¨ãƒ›ã‚¹ãƒˆã§ä¸€è‡´ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã¯ãšã§ã™ã€‚

ã“ã‚Œã‚‰ã‚’çµ„ã¿åˆã‚ã›ã‚Œã°ã€ãƒ›ã‚¹ãƒˆãƒ»ã‚³ãƒ³ãƒ†ãƒŠé–“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©ã‚’ãã‚Œãªã‚Šã«ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã§ãã‚‹ã“ã¨ã§ã—ã‚‡ã†ã€‚
---------------
https://stgraber.org/2017/06/15/custom-user-mappings-in-lxd-containers/
Introduction
As you may know, LXD uses unprivileged containers by default.
The difference between an unprivileged container and a privileged one is whether the root user in the container is the â€œrealâ€ root user (uid 0 at the kernel level).

The way unprivileged containers are created is by taking a set of normal UIDs and GIDs from the host, usually at least 65536 of each (to be POSIX compliant) and mapping those into the container.

The most common example and what most LXD users will end up with by default is a map of 65536 UIDs and GIDs, with a host base id of 100000. This means that root in the container (uid 0) will be mapped to the host uid 100000 and uid 65535 in the container will be mapped to uid 165535 on the host. UID/GID 65536 and higher in the container arenâ€™t mapped and will return an error if you attempt to use them.

From a security point of view, that means that anything which is not owned by the users and groups mapped into the container will be inaccessible. Any such resource will show up as being owned by uid/gid â€œ-1â€ (rendered as 65534 or nobody/nogroup in userspace). It also means that should there be a way to escape the container, even root in the container would find itself with just as much privileges on the host as a nobody user.

LXD does offer a number of options related to unprivileged configuration:

Increasing the size of the default uid/gid map
Setting up per-container maps
Punching holes into the map to expose host users and groups
Increasing the size of the default map
As mentioned above, in most cases, LXD will have a default map thatâ€™s made of 65536 uids/gids.

In most cases you wonâ€™t have to change that. There are however a few cases where you may have to:

You need access to uid/gid higher than 65535.
This is most common when using network authentication inside of your containers.
You want to use per-container maps.
In which case youâ€™ll need 65536 available uid/gid per container.
You want to punch some holes in your containerâ€™s map and need access to host uids/gids.
The default map is usually controlled by the â€œshadowâ€ set of utilities and files. On systems where thatâ€™s the case, the â€œ/etc/subuidâ€ and â€œ/etc/subgidâ€ files are used to configure those maps.

On systems that do not have a recent enough version of the â€œshadowâ€ package. LXD will assume that it doesnâ€™t have to share uid/gid ranges with anything else and will therefore assume control of a billion uids and gids, starting at the host uid/gid 100000.

But the common case, is a system with a recent version of shadow.
An example of what the configuration may look like is:

stgraber@castiana:~$ cat /etc/subuid
lxd:100000:65536
root:100000:65536

stgraber@castiana:~$ cat /etc/subgid
lxd:100000:65536
root:100000:65536
The maps for â€œlxdâ€ and â€œrootâ€ should always be kept in sync. LXD itself is restricted by the â€œrootâ€ allocation. The â€œlxdâ€ entry is used to track what needs to be removed if LXD is uninstalled.

Now if you want to increase the size of the map available to LXD. Simply edit both of the files and bump the last value from 65536 to whatever size you need. I tend to bump it to a billion just so I donâ€™t ever have to think about it again:

stgraber@castiana:~$ cat /etc/subuid
lxd:100000:1000000000
root:100000:1000000000

stgraber@castiana:~$ cat /etc/subgid
lxd:100000:1000000000
root:100000:100000000
After altering those files, you need to restart LXD to have it detect the new map:

root@vorash:~# systemctl restart lxd
root@vorash:~# cat /var/log/lxd/lxd.log
lvl=info msg="LXD 2.14 is starting in normal mode" path=/var/lib/lxd t=2017-06-14T21:21:13+0000
lvl=warn msg="CGroup memory swap accounting is disabled, swap limits will be ignored." t=2017-06-14T21:21:13+0000
lvl=info msg="Kernel uid/gid map:" t=2017-06-14T21:21:13+0000
lvl=info msg=" - u 0 0 4294967295" t=2017-06-14T21:21:13+0000
lvl=info msg=" - g 0 0 4294967295" t=2017-06-14T21:21:13+0000
lvl=info msg="Configured LXD uid/gid map:" t=2017-06-14T21:21:13+0000
lvl=info msg=" - u 0 1000000 1000000000" t=2017-06-14T21:21:13+0000
lvl=info msg=" - g 0 1000000 1000000000" t=2017-06-14T21:21:13+0000
lvl=info msg="Connecting to a remote simplestreams server" t=2017-06-14T21:21:13+0000
lvl=info msg="Expiring log files" t=2017-06-14T21:21:13+0000
lvl=info msg="Done expiring log files" t=2017-06-14T21:21:13+0000
lvl=info msg="Starting /dev/lxd handler" t=2017-06-14T21:21:13+0000
lvl=info msg="LXD is socket activated" t=2017-06-14T21:21:13+0000
lvl=info msg="REST API daemon:" t=2017-06-14T21:21:13+0000
lvl=info msg=" - binding Unix socket" socket=/var/lib/lxd/unix.socket t=2017-06-14T21:21:13+0000
lvl=info msg=" - binding TCP socket" socket=[::]:8443 t=2017-06-14T21:21:13+0000
lvl=info msg="Pruning expired images" t=2017-06-14T21:21:13+0000
lvl=info msg="Updating images" t=2017-06-14T21:21:13+0000
lvl=info msg="Done pruning expired images" t=2017-06-14T21:21:13+0000
lvl=info msg="Done updating images" t=2017-06-14T21:21:13+0000
root@vorash:~#
As you can see, the configured map is logged at LXD startup and can be used to confirm that the reconfiguration worked as expected.

Youâ€™ll then need to restart your containers to have them start using your newly expanded map.

Per container maps
Provided that you have a sufficient amount of uid/gid allocated to LXD, you can configure your containers to use their own, non-overlapping allocation of uids and gids.

This can be useful for two reasons:

You are running software which alters kernel resource ulimits.
Those user-specific limits are tied to a kernel uid and will cross container boundaries leading to hard to debug issues where one container can perform an action but all others are then unable to do the same.
You want to know that should there be a way for someone in one of your containers to somehow get access to the host that they still wonâ€™t be able to access or interact with any of the other containers.
The main downsides to using this feature are:

Itâ€™s somewhat wasteful with using 65536 uids and gids per container.
That being said, youâ€™d still be able to run over 60000 isolated containers before running out of system uids and gids.
Itâ€™s effectively impossible to share storage between two isolated containers as everything written by one will be seen as -1 by the other. There is ongoing work around virtual filesystems in the kernel that will eventually let us get rid of that limitation.
To have a container use its own distinct map, simply run:

stgraber@castiana:~$ lxc config set test security.idmap.isolated true
stgraber@castiana:~$ lxc restart test
stgraber@castiana:~$ lxc config get test volatile.last_state.idmap
[{"Isuid":true,"Isgid":false,"Hostid":165536,"Nsid":0,"Maprange":65536},{"Isuid":false,"Isgid":true,"Hostid":165536,"Nsid":0,"Maprange":65536}]
The restart step is needed to have LXD remap the entire filesystem of the container to its new map.
Note that this step will take a varying amount of time depending on the number of files in the container and the speed of your storage.

As can be seen above, after restart, the container is shown to have its own map of 65536 uids/gids.

If you want LXD to allocate more than the default 65536 uids/gids to an isolated container, you can bump the size of the allocation with:

stgraber@castiana:~$ lxc config set test security.idmap.size 200000
stgraber@castiana:~$ lxc restart test
stgraber@castiana:~$ lxc config get test volatile.last_state.idmap
[{"Isuid":true,"Isgid":false,"Hostid":165536,"Nsid":0,"Maprange":200000},{"Isuid":false,"Isgid":true,"Hostid":165536,"Nsid":0,"Maprange":200000}]
If youâ€™re trying to allocate more uids/gids than are left in LXDâ€™s allocation, LXD will let you know:

stgraber@castiana:~$ lxc config set test security.idmap.size 2000000000
error: Not enough uid/gid available for the container.
Direct user/group mapping
The fact that all uids/gids in an unprivileged container are mapped to a normally unused range on the host means that sharing of data between host and container is effectively impossible.

Now, what if you want to share your userâ€™s home directory with a container?

The obvious answer to that is to define a new â€œdiskâ€ entry in LXD which passes your home directory to the container:

stgraber@castiana:~$ lxc config device add test home disk source=/home/stgraber path=/home/ubuntu
Device home added to test
So that was pretty easy, but did it work?

stgraber@castiana:~$ lxc exec test -- bash
root@test:~# ls -lh /home/
total 529K
drwx--x--x 45 nobody nogroup 84 Jun 14 20:06 ubuntu
No. The mount is clearly there, but itâ€™s completely inaccessible to the container.
To fix that, we need to take a few extra steps:

Allow LXDâ€™s use of our user uid and gid
Restart LXD to have it load the new map
Set a custom map for our container
Restart the container to have the new map apply
stgraber@castiana:~$ printf "lxd:$(id -u):1\nroot:$(id -u):1\n" | sudo tee -a /etc/subuid
lxd:201105:1
root:201105:1

stgraber@castiana:~$ printf "lxd:$(id -g):1\nroot:$(id -g):1\n" | sudo tee -a /etc/subgid
lxd:200512:1
root:200512:1

stgraber@castiana:~$ sudo systemctl restart lxd

stgraber@castiana:~$ printf "uid $(id -u) 1000\ngid $(id -g) 1000" | lxc config set test raw.idmap -

stgraber@castiana:~$ lxc restart test
At which point, things should be working in the container:

stgraber@castiana:~$ lxc exec test -- su ubuntu -l
ubuntu@test:~$ ls -lh
total 119K
drwxr-xr-x 5  ubuntu ubuntu 8 Feb 18 2016 data
drwxr-x--- 4  ubuntu ubuntu 6 Jun 13 17:05 Desktop
drwxr-xr-x 3  ubuntu ubuntu 28 Jun 13 20:09 Downloads
drwx------ 84 ubuntu ubuntu 84 Sep 14 2016 Maildir
drwxr-xr-x 4  ubuntu ubuntu 4 May 20 15:38 snap
ubuntu@test:~$

Conclusion
User namespaces, the kernel feature that makes those uid/gid mappings possible is a very powerful tool which finally made containers on Linux safe by design. It is however not the easiest thing to wrap your head around and all of that uid/gid map math can quickly become a major issue.

In LXD weâ€™ve tried to expose just enough of those underlying features to be useful to our users while doing the actual mapping math internally. This makes things like the direct user/group mapping above significantly easier than it otherwise would be.

Going forward, weâ€™re very interested in some of the work around uid/gid remapping at the filesystem level, this would let us decouple the on-disk user/group map from that used for processes, making it possible to share data between differently mapped containers and alter the various maps without needing to also remap the entire filesystem.

Extra information
The main LXD website is at: https://linuxcontainers.org/lxd
Development happens on Github at: https://github.com/lxc/lxd
Discussion forun: https://discuss.linuxcontainers.org
Mailing-list support happens on: https://lists.linuxcontainers.org
IRC support happens in: #lxcontainers on irc.freenode.net
Try LXD online: https://linuxcontainers.org/lxd/try-it


----
https://stackoverflow.com/questions/71738168/linux-container-failed-to-set-up-id-mapping


your mappings look quite wrong... syntax should be :

/etc/subuid

[local unprivileged user on the host that will run the container]:[uid on the host start at(this is inexisting uid on the host and it is ok)]:[number/range of uid to map]

e.g : toto:100000:65535

/etc/subuid

[local unprivileged group on the host that will run the container]:[guid on the host start at(this is inexisting gid on the host and it is ok)]:[number/range of uid to map]

e.g : toto:100000:65535

config

then, in the config file of the container, if you want to restrict the access further, you can do some specific mappings:

lxc.idmap = u 0 100000 1

lxc.idmap = g 0 100000 1

lxc.idmap = u 33 100033 1

lxc.idmap = g 33 100033 1

will map the uid and gid 0 (root) in the container to uid and gid 100000 on the host

will map the uid and gid 33 (www-data) in the container to uid and gid 100033 on the host

--------------

LXDã®ã‚³ãƒ³ãƒ†ãƒŠä¸­ã§ãƒ›ã‚¹ãƒˆã®ãƒ›ãƒ¼ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’èª­ã¿æ›¸ãã™ã‚‹æ–¹æ³•
https://qiita.com/m-shibata/items/2969ab84bab9235d25f0
https://ubuntu.com/blog/mounting-your-home-directory-in-lxd
Mounting your home directory in LXD
As of LXD stable 2.0.8 and feature release 2.6, LXD has support for various UID and GID map related manipulaions. A common question is: â€œHow do I bind-mount my home directory into a container?â€ and before the answer was â€œwell, itâ€™s complicated but you can do it; itâ€™s slightly less complicated if you do it in privleged containersâ€. However, with this feature, now you can do it very easily in unprivileged containers.

First, find out your uid on the host:

$ id
uid=1000(tycho) gid=1000(tycho) groups=1000(tycho),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),112(lpadmin),124(sambashare),129(libvirtd),149(lxd),150(sbuild)
On standard Ubuntu hosts, the uid of the first user is 1000. Now, we need to allow LXD to remap to remap this id; youâ€™ll need an additional entry for root to do this:

$ echo 'root:1000:1' | sudo tee -a /etc/subuid /etc/subgid
Now, create a container, and set the idmap up to map both uid and gid 1000 to uid and gid 1000 inside the container.

$ lxc init ubuntu-daily:z zesty
Creating zesty

$ lxc config set zesty raw.idmap 'both 1000 1000'
Finally, set up your home directory to be mounted in the container:

$ lxc config device add zesty homedir disk source=/home/tycho path=/home/ubuntu
And leave an insightful message for users of the container:

$ echo 'meshuggah rocks' >> message
Finally, start your container and read the message:

$ lxc start zesty
$ lxc exec zesty cat /home/ubuntu/message
meshuggah rocks
And enjoy the insight offered to you by your home directory ðŸ™‚


Sunday, August 28, 2022

-->

See more in [Container with LXC/LXD (3)]({{<ref "note-00054.md" >}}).

<!-- vim: set sw=4 sts=4 ai si et tw=79 ft=markdown: -->
